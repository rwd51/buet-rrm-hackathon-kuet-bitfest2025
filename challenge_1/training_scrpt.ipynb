{"cells":[{"cell_type":"code","execution_count":5,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-12-21T15:42:10.244063Z","iopub.status.busy":"2024-12-21T15:42:10.243762Z","iopub.status.idle":"2024-12-21T15:42:13.582875Z","shell.execute_reply":"2024-12-21T15:42:13.581965Z","shell.execute_reply.started":"2024-12-21T15:42:10.244039Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.3)\n","Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.2.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\n","Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.1.4)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.5)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n","Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\n","Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.24.7)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.16.1)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.10.5)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.1)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.11.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n"]}],"source":["!pip install evaluate"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-12-21T15:43:04.600583Z","iopub.status.busy":"2024-12-21T15:43:04.600188Z","iopub.status.idle":"2024-12-21T15:43:04.608074Z","shell.execute_reply":"2024-12-21T15:43:04.607283Z","shell.execute_reply.started":"2024-12-21T15:43:04.600555Z"},"trusted":true},"outputs":[],"source":["# Import required libraries\n","import pandas as pd\n","import torch\n","import numpy as np\n","from torch.utils.data import Dataset\n","from transformers import (\n","    MT5ForConditionalGeneration,\n","    T5Tokenizer,\n","    Trainer,\n","    TrainingArguments,\n","    DataCollatorForSeq2Seq,\n","    EarlyStoppingCallback\n",")\n","from sklearn.model_selection import train_test_split\n","import re\n","from typing import List, Dict, Tuple\n","import logging\n","from datasets import load_dataset\n","from evaluate import load\n","import random\n","from tqdm.auto import tqdm\n","\n","# Set up logging\n","logging.basicConfig(level=logging.INFO)\n","logger = logging.getLogger(__name__)\n","\n","# Set random seeds for reproducibility\n","def set_seed(seed=42):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","\n","set_seed()"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-12-21T15:43:07.694684Z","iopub.status.busy":"2024-12-21T15:43:07.694395Z","iopub.status.idle":"2024-12-21T15:43:07.700645Z","shell.execute_reply":"2024-12-21T15:43:07.699595Z","shell.execute_reply.started":"2024-12-21T15:43:07.694662Z"},"trusted":true},"outputs":[],"source":["# Data preprocessing and augmentation\n","class TextPreprocessor:\n","    @staticmethod\n","    def clean_text(text: str) -> str:\n","        \"\"\"Clean and normalize text\"\"\"\n","        text = str(text).strip()\n","        text = re.sub(r'\\s+', ' ', text)  # Remove extra whitespace\n","        text = text.lower()  # Convert to lowercase\n","        return text\n","    \n","    @staticmethod\n","    def create_augmented_samples(text: str) -> List[str]:\n","        \"\"\"Create augmented versions of the input text\"\"\"\n","        augmented = []\n","        \n","        # Original text\n","        augmented.append(text)\n","        \n","        # Add space between characters randomly\n","        chars = list(text)\n","        if len(chars) > 3:\n","            idx = random.randint(1, len(chars)-2)\n","            chars.insert(idx, ' ')\n","            augmented.append(''.join(chars))\n","        \n","        # Remove random character\n","        if len(text) > 3:\n","            idx = random.randint(1, len(text)-2)\n","            augmented.append(text[:idx] + text[idx+1:])\n","        \n","        return augmented"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-12-21T15:43:09.997001Z","iopub.status.busy":"2024-12-21T15:43:09.996708Z","iopub.status.idle":"2024-12-21T15:43:10.003138Z","shell.execute_reply":"2024-12-21T15:43:10.002322Z","shell.execute_reply.started":"2024-12-21T15:43:09.996977Z"},"trusted":true},"outputs":[],"source":["# Custom Dataset\n","class BanglishBengaliDataset(Dataset):\n","    def __init__(self, texts: List[str], labels: List[str], tokenizer, max_length: int = 128):\n","        self.texts = texts\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","        self.preprocessor = TextPreprocessor()\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        text = self.preprocessor.clean_text(self.texts[idx])\n","        label = str(self.labels[idx])\n","\n","        # Add task prefix\n","        text = \"transliterate Bengali: \" + text\n","\n","        # Tokenize inputs\n","        inputs = self.tokenizer(\n","            text,\n","            padding=\"max_length\",\n","            truncation=True,\n","            max_length=self.max_length,\n","            return_tensors=\"pt\"\n","        )\n","\n","        # Tokenize labels\n","        with self.tokenizer.as_target_tokenizer():\n","            labels = self.tokenizer(\n","                label,\n","                padding=\"max_length\",\n","                truncation=True,\n","                max_length=self.max_length,\n","                return_tensors=\"pt\"\n","            )\n","\n","        return {\n","            \"input_ids\": inputs[\"input_ids\"].squeeze(),\n","            \"attention_mask\": inputs[\"attention_mask\"].squeeze(),\n","            \"labels\": labels[\"input_ids\"].squeeze()\n","        }"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-12-21T15:43:11.941036Z","iopub.status.busy":"2024-12-21T15:43:11.940734Z","iopub.status.idle":"2024-12-21T15:43:11.946245Z","shell.execute_reply":"2024-12-21T15:43:11.945334Z","shell.execute_reply.started":"2024-12-21T15:43:11.941010Z"},"trusted":true},"outputs":[],"source":["# Metrics calculation\n","def compute_metrics(pred):\n","    \"\"\"Calculate metrics for evaluation\"\"\"\n","    predictions, labels = pred\n","    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n","    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","    \n","    # Calculate character error rate (CER)\n","    cer_metric = load(\"cer\")\n","    cer = cer_metric.compute(predictions=decoded_preds, references=decoded_labels)\n","    \n","    # Calculate exact match score\n","    exact_matches = sum(1 for p, l in zip(decoded_preds, decoded_labels) if p.strip() == l.strip())\n","    exact_match_score = exact_matches / len(decoded_preds)\n","    \n","    return {\n","        \"cer\": cer,\n","        \"exact_match\": exact_match_score\n","    }"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-12-21T15:43:13.987790Z","iopub.status.busy":"2024-12-21T15:43:13.987479Z","iopub.status.idle":"2024-12-21T15:43:14.000886Z","shell.execute_reply":"2024-12-21T15:43:14.000031Z","shell.execute_reply.started":"2024-12-21T15:43:13.987762Z"},"trusted":true},"outputs":[],"source":["# Main Transliterator class\n","class ImprovedBanglishBengaliTransliterator:\n","    def __init__(\n","        self,\n","        model_name: str = \"google/mt5-large\",\n","        max_length: int = 128\n","    ):\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        print(f\"Using device: {self.device}\")\n","        \n","        self.model_name = model_name\n","        self.max_length = max_length\n","        \n","        # Initialize tokenizer with special tokens\n","        self.tokenizer = T5Tokenizer.from_pretrained(model_name)\n","        \n","        # Initialize model with better initialization\n","        self.model = MT5ForConditionalGeneration.from_pretrained(model_name)\n","        self.model.to(self.device)\n","\n","    def prepare_data(\n","        self,\n","        data_path: str,\n","        test_size: float = 0.1,\n","        apply_augmentation: bool = True\n","    ) -> Tuple[Dataset, Dataset]:\n","        \"\"\"Load and prepare the dataset with augmentation\"\"\"\n","        logger.info(\"Loading dataset...\")\n","        df = pd.read_csv(data_path)\n","        \n","        print(f\"Initial dataset size: {len(df)}\")\n","        \n","        # Clean the data\n","        df = df.dropna()\n","        preprocessor = TextPreprocessor()\n","        \n","        # Clean texts\n","        df['rm'] = df['rm'].apply(preprocessor.clean_text)\n","        df['bn'] = df['bn'].apply(str.strip)\n","        \n","        # Apply augmentation if enabled\n","        if apply_augmentation:\n","            augmented_data = []\n","            for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Augmenting data\"):\n","                augmented_texts = preprocessor.create_augmented_samples(row['rm'])\n","                for aug_text in augmented_texts:\n","                    augmented_data.append({\n","                        'rm': aug_text,\n","                        'bn': row['bn']\n","                    })\n","            \n","            df_augmented = pd.DataFrame(augmented_data)\n","            df = pd.concat([df, df_augmented], ignore_index=True)\n","            print(f\"Dataset size after augmentation: {len(df)}\")\n","        \n","        # Split the dataset\n","        train_texts, val_texts, train_labels, val_labels = train_test_split(\n","            df['rm'].values, df['bn'].values,\n","            test_size=test_size,\n","            random_state=42\n","        )\n","        \n","        print(f\"Training set size: {len(train_texts)}\")\n","        print(f\"Validation set size: {len(val_texts)}\")\n","\n","        # Create datasets\n","        train_dataset = BanglishBengaliDataset(\n","            train_texts, train_labels, self.tokenizer, self.max_length\n","        )\n","        val_dataset = BanglishBengaliDataset(\n","            val_texts, val_labels, self.tokenizer, self.max_length\n","        )\n","\n","        return train_dataset, val_dataset\n","\n","    def train(\n","        self,\n","        train_dataset: Dataset,\n","        val_dataset: Dataset,\n","        output_dir: str = \"./improved-banglish-bengali-model\",\n","        num_train_epochs: int = 5,\n","        per_device_train_batch_size: int = 8,\n","        gradient_accumulation_steps: int = 2,\n","        learning_rate: float = 2e-5,\n","        warmup_ratio: float = 0.1\n","    ):\n","        \"\"\"Train the model with improved training strategy\"\"\"\n","        logger.info(\"Starting training...\")\n","        \n","        # Calculate warmup steps\n","        num_update_steps_per_epoch = len(train_dataset) // (per_device_train_batch_size * gradient_accumulation_steps)\n","        max_steps = num_train_epochs * num_update_steps_per_epoch\n","        warmup_steps = int(max_steps * warmup_ratio)\n","        \n","        training_args = TrainingArguments(\n","            output_dir=output_dir,\n","            num_train_epochs=num_train_epochs,\n","            per_device_train_batch_size=per_device_train_batch_size,\n","            gradient_accumulation_steps=gradient_accumulation_steps,\n","            per_device_eval_batch_size=per_device_train_batch_size * 2,\n","            evaluation_strategy=\"steps\",\n","            eval_steps=num_update_steps_per_epoch // 2,\n","            save_strategy=\"steps\",\n","            save_steps=num_update_steps_per_epoch,\n","            learning_rate=learning_rate,\n","            warmup_steps=warmup_steps,\n","            weight_decay=0.01,\n","            logging_dir=f\"{output_dir}/logs\",\n","            logging_steps=100,\n","            load_best_model_at_end=True,\n","            metric_for_best_model=\"eval_cer\",\n","            greater_is_better=False,\n","            fp16=torch.cuda.is_available(),\n","            report_to=\"tensorboard\"\n","        )\n","\n","        data_collator = DataCollatorForSeq2Seq(\n","            tokenizer=self.tokenizer,\n","            model=self.model,\n","            padding=True\n","        )\n","\n","        trainer = Trainer(\n","            model=self.model,\n","            args=training_args,\n","            train_dataset=train_dataset,\n","            eval_dataset=val_dataset,\n","            data_collator=data_collator,\n","            compute_metrics=compute_metrics,\n","            callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n","        )\n","\n","        trainer.train()\n","        \n","        # Save the final model and tokenizer\n","        self.model.save_pretrained(output_dir)\n","        self.tokenizer.save_pretrained(output_dir)\n","        logger.info(f\"Model saved to {output_dir}\")\n","\n","    def transliterate(self, text: str, num_beams: int = 5) -> str:\n","        \"\"\"Transliterate text with beam search\"\"\"\n","        self.model.eval()\n","        \n","        # Preprocess input\n","        text = TextPreprocessor.clean_text(text)\n","        text = \"transliterate Bengali: \" + text\n","        \n","        # Tokenize\n","        inputs = self.tokenizer(\n","            text,\n","            padding=True,\n","            truncation=True,\n","            max_length=self.max_length,\n","            return_tensors=\"pt\"\n","        ).to(self.device)\n","\n","        # Generate with beam search\n","        with torch.no_grad():\n","            outputs = self.model.generate(\n","                input_ids=inputs[\"input_ids\"],\n","                attention_mask=inputs[\"attention_mask\"],\n","                max_length=self.max_length,\n","                num_beams=num_beams,\n","                length_penalty=1.0,\n","                early_stopping=True,\n","                no_repeat_ngram_size=2,\n","                do_sample=False\n","            )\n","\n","        # Decode prediction\n","        predicted_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n","        return predicted_text"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-12-21T15:43:17.716115Z","iopub.status.busy":"2024-12-21T15:43:17.715824Z","iopub.status.idle":"2024-12-21T15:43:19.748099Z","shell.execute_reply":"2024-12-21T15:43:19.747280Z","shell.execute_reply.started":"2024-12-21T15:43:17.716093Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dfc6cf1d0f2148cdb8f5ea7072811a98","version_major":2,"version_minor":0},"text/plain":["README.md:   0%|          | 0.00/300 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1890c25f94ab4bf288410e8721d4ce02","version_major":2,"version_minor":0},"text/plain":["train-00000-of-00001.parquet:   0%|          | 0.00/333k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cdf6fa26e76942c3a713d4c85180e830","version_major":2,"version_minor":0},"text/plain":["Generating train split:   0%|          | 0/5006 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# Loading the dataset\n","ds = load_dataset(\"SKNahin/bengali-transliteration-data\")\n","\n","# Combining all splits into one\n","combined_df = pd.concat([split.to_pandas() for split in ds.values()], ignore_index=True)\n","\n","# Saving the combined dataset to a single CSV file\n","combined_df.to_csv(\"data.csv\", index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-12-21T15:43:26.617983Z","iopub.status.busy":"2024-12-21T15:43:26.617691Z","iopub.status.idle":"2024-12-21T15:44:40.748200Z","shell.execute_reply":"2024-12-21T15:44:40.746895Z","shell.execute_reply.started":"2024-12-21T15:43:26.617959Z"},"trusted":true},"outputs":[],"source":["# Training and evaluation\n","# Initialize the transliterator\n","transliterator = ImprovedBanglishBengaliTransliterator()\n","\n","# Prepare data with augmentation\n","train_dataset, val_dataset = transliterator.prepare_data(\n","    \"/kaggle/working/data.csv\",\n","    apply_augmentation=True\n",")\n","\n","# Train the model\n","transliterator.train(\n","    train_dataset,\n","    val_dataset,\n","    num_train_epochs=5,\n","    per_device_train_batch_size=8\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":6348829,"sourceId":10262833,"sourceType":"datasetVersion"}],"dockerImageVersionId":30823,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
